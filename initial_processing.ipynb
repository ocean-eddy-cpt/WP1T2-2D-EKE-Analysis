{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script takes satellite data from catalog, \n",
    "# parses to place on regularly gridded array, and interpolates nan across desired gaps in space and time\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr \n",
    "import time \n",
    "from intake import open_catalog\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from scipy import integrate\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle \n",
    "from altimetry_tools import Haversine, nan_helper, interp_nans, parse_grid_tracks, smooth_tracks, coarsen, specsharp, spectra_slopes\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# -- main PARAMETERS --- \n",
    "this_sat = 'j3'              # mission to consider \n",
    "hor_grid_spacing = 10        # km, grid to which to interpolate tracks\n",
    "interp_cutoff = 5            # number of acceptable nan gaps in grid cell units across which to interpolate\n",
    "coarsening_factor0 = 7       # (*hor_grid_spacing = coarsened grid size) this factor should be multiplied by hor_grid_spacing \n",
    "nyquist_wavelength = np.pi   # factor relative to coarsening factor (what scales do we want to resolve...pi times the grid scale)\n",
    "f_type = 'sharp'             # filter type (gaussian or sharp)\n",
    "# -----------------------\n",
    "# -- coastline file \n",
    "x4 = xr.open_dataset('/home/jovyan/along-track-altimetry-analysis/misc/coastlines_global.nc')  \n",
    "# -- deformation radius \n",
    "# Chelton et al. 1998 'global variability of the first baroclinic rossby radius of deformation'\n",
    "c98 = xr.open_dataset('/home/jovyan/along-track-altimetry-analysis/misc/global_deformation_radius_chelton_1998.nc')  \n",
    "c98 = c98['values'].data\n",
    "# (http://www-po.coas.oregonstate.edu/research/po/research/rossby_radius/index.html#anchor2)\n",
    "# -- catalog of available satellites \n",
    "cat = open_catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore/master/intake-catalogs/ocean/altimetry.yaml\")# load individual satellite \n",
    "# ['al', 'alg', 'c2', 'e1', 'e1g', 'e2', 'en', 'enn', 'g2', 'h2', 'j1', 'j1g', 'j1n', 'j2', 'j2g', 'j2n', 'j3', 's3a', 's3b', 'tp', 'tpn']\n",
    "\n",
    "# -- lat/lon boundaries (permits subsetting to speed processing time)\n",
    "lon_w = 0\n",
    "lon_e = 360\n",
    "lat_s = -65\n",
    "lat_n = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# RUN ONLY if parsed tracks file doesn't exist (should have a parsed file for each satellite)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# load individual sat (your choice) and convert to dataframe from dataset \n",
    "ds2_0 = cat[this_sat].to_dask()\n",
    "%time ds2 = ds2_0[['latitude', 'longitude', 'sla_unfiltered', 'sla_filtered', 'track', 'cycle', 'mdt']].reset_coords().astype('f4').load()\n",
    "%time df2 = ds2.to_dataframe()\n",
    "df2_s = df2[(df2['longitude'] > lon_w) & (df2['longitude'] < lon_e) & (df2['latitude'] > lat_s) & (df2['latitude'] < lat_n)]\n",
    "in_tracks2 = np.unique(df2_s['track'])\n",
    "print('tracks in domain')\n",
    "print(in_tracks2)\n",
    "\n",
    "test = df2_s[df2_s['track'] == 11]  # 9 is standard\n",
    "p = test.index\n",
    "ts = (p - np.datetime64('1970-01-01T00:00:00Z')) / np.timedelta64(1, 's')\n",
    "test2 = np.where(np.diff(ts) > 24*60*60)[0]\n",
    "repeat_time = p[test2[3]] - p[test2[2]]\n",
    "print('repeat time = ' + str(repeat_time))\n",
    "\n",
    "test0 = df2_s[df2_s['track'] == in_tracks2[10]]  # 11\n",
    "in_cycles0 = np.unique(test0['cycle'])\n",
    "test1 = test0[test0['cycle'] == in_cycles0[0]]  # 20\n",
    "ii = 5\n",
    "d = Haversine(test1['latitude'][ii], test1['longitude'][ii], test1['latitude'][ii-1], test1['longitude'][ii-1])\n",
    "print('nominal grid spacing = ' + str(d) + 'km')\n",
    "\n",
    "# -- PARSE_grid_tracks (interpolate nans and group by track) (sla dimensions = [track_number][cycle, along_track_grid])\n",
    "f_v_uf = 1  # if 1, use filtered product from aviso \n",
    "%time lon_t, lat_t, track_t, adt, sla, dist, lon_record, lat_record, time_record, track_record \\\n",
    "    = parse_grid_tracks(in_tracks2, df2_s, hor_grid_spacing, interp_cutoff, f_v_uf)\n",
    "\n",
    "del sla # make more memory space \n",
    "# ---------------   \n",
    "# -- export to pickle so that we don't have to run/parse everytime \n",
    "# ---------------\n",
    "save_p = 1\n",
    "if save_p > 0:\n",
    "    outputs = {'lon_t': lon_t, 'lat_t': lat_t, 'track_t': track_t, 'adt': adt, \\\n",
    "               'dist': dist, 'lon_record': lon_record, 'lat_record': lat_record, \\\n",
    "               'time_record': time_record, 'track_record': track_record, \\\n",
    "               'repeat_time': repeat_time, 'interp_cutoff': interp_cutoff}\n",
    "    pickle.dump(outputs, open(this_sat + '/' + this_sat + '_parsed_tracks_adt.p', 'wb'))\n",
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# RUN AFTER parse_grid_tracks is completed  (should have a parsed file for each satellite)\n",
    "# now interpolate in time (input should be output file from above cell)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "load_sat = pickle.load(open(this_sat + '/' + this_sat +'_parsed_tracks_adt.p', 'rb'))\n",
    "adt = load_sat['adt']\n",
    "# -- interpolate in time \n",
    "interp_cutoff_t = 5  # time step to interpolate across (number samples, not a unit of time)\n",
    "adt_time = []\n",
    "sla_time = []\n",
    "# - loop over tracks \n",
    "for i in tqdm(range(len(adt))):\n",
    "    # - loop over along-track gridpoints \n",
    "    # this_interp_t_sla = np.nan * np.ones(np.shape(sla[i]))\n",
    "    this_interp_t_adt = np.nan * np.ones(np.shape(adt[i]))\n",
    "    for j in range(np.shape(adt[i])[1]): \n",
    "        # this_data_1 = sla[i][:, j]\n",
    "        this_data_2 = adt[i][:, j]\n",
    "        # if np.sum(np.isnan(this_data_1)) > 0:\n",
    "        #     nans, x = nan_helper(this_data_1)\n",
    "        #     this_interp_t_sla[:, j] = interp_nans(this_data_1, nans, x, interp_cutoff_t)\n",
    "        # else:\n",
    "        #     this_interp_t_sla[:, j] = this_data_1.copy()\n",
    "        if np.sum(np.isnan(this_data_2)) > 0:\n",
    "            nans, x = nan_helper(this_data_2)\n",
    "            this_interp_t_adt[:, j] = interp_nans(this_data_2, nans, x, interp_cutoff_t)\n",
    "        else:\n",
    "            this_interp_t_adt[:, j] = this_data_2.copy()\n",
    "    # sla_time.append(this_interp_t_sla)\n",
    "    adt_time.append(this_interp_t_adt)\n",
    "    \n",
    "save_p = 1\n",
    "if save_p > 0:\n",
    "    outputs = {'adt': adt_time}\n",
    "    pickle.dump(outputs, open(this_sat + '/' + this_sat + '_parsed_tracks_adt_interp_time_only.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
